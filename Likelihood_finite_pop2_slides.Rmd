---
title: "(Monte Carlo) Likelihood theory for finite populations." #(: a rematch in the computational era.) model calibration; model and design efficiency; 
author: "Markus Stein"
date: "July 11, 2019"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Real life meaning of probability
What are the set of all possible outcomes?  

- K. Pearson - probability distributions are observable by collecting **lots of data**. 

- W. Gosset - attempted to describe experimental design.

- R. A. Fisher - set of all possible random assignments, **permutation**.
    * How big is the set of all permutations?
    * And in observational studies?
    
### **Finite population** meaning of probability is well estabilished!


## Finite population likelihood ideas
Assume a random variable $Y \sim Bernoulli(\pi)$, assume that we observe a population $y_1, \ldots, y_N$ from $Y$.


### Population likelihood
If we observed all $y_1, \ldots, y_N$, the **population likelihood** is
$$ L_{pop}(\theta) = \prod_{i=1}^{N} \pi^{y_i} \: (1-\pi)^{(1-y_i)}$$



## Sample likelihood
- Now, assume that we can't observe the $N$ elements,
    + But a sample of size $n \ll N$.  
    

- Then, we define the sample likelihood  
$$ L_{sample}(\theta) = \prod_{i=1}^{n} \pi^{y_i} \: (1-\pi)^{(1-y_i)} $$

- Can $n\rightarrow \infty$???

## Likelihood approximation
log-likelihood expression

- Laplace  
- EM algorithm  
- Bayesian  


## Missing data problem
- Consider the $N - n$ non-observed units...

- To generate not observed units given the observed.



## Monte Carlo likelihood approximation





## Latent variables
- longitudinal

- ecological fallacy


## mixed models

## model calibration

## Model and design efficiency

## Asymptotics?

## A bit of 'Asymptotics'



model calibration.... design- model-eficciency... 



### Title: 
(Monte Carlo) Likelihood theory for finite populations: the binary response example.


### Abstract:
Likelihood-based approaches for combining aggregated data with individual-level 
data in contingency tables will be considered in this presentation. Such data 
structure emerges in a wide body of literature, mainly by design or model-choice, 
<!-- e.g. by disclosure limitation of  -->
<!-- private unaggregated data survey agencies usually release group level information,  -->
<!-- and the only reliable way to reduce potencial biases (commonly called "ecological  -->
<!-- fallacy") consists in supplementing these marginal distributions by unit-record  -->
<!-- information. Similar problems arise when researchers working on complex survey  -->
<!-- designs may take advantage of available marginal population information to improve  -->
<!-- efficiency. -->
Under the missing data perspective, the finite population can be viewed as the union 
of two subsets, the sampled and the not-sampled individuals. The problem here is that 
likelihood-based inferences tend to be very expensive because the true likelihood 
function has a form of an expectation with respect to all possible values of the 
not-sampled individuals. We propose an approximation to the entire likelihood by 
estimating the expectation via a sample average. Heuristics for likelihood convergence 
is given. We show aplicability of the method for estimation in a linear logistic models, 
and simulation  results show an advantage in efficiency by using our proposed method.



In these context maximum likelihood inference can be computationally infeasible because calculating the likelihood function involves summing over the set of all possible tables. To avoid its calculations a binomial approximation to the true likelihood is presented. We also consider the stochastic EM algorithm and related methods that involve sampling the possible tables. Points of interest are accuracy and computational burden particularly in rare outcome problems. Moreover, we show that good approximations to the likelihood function can be carried out by sampling possible tables and maximising a sample version of this. Simulation results for estimation in a linear logistic model show large improvements by using the estimated likelihood.


This paper is about likelihood approximation for combining aggregated-data and 
case-control designs with categorical covariates. 


We consider strategy to combine aggregate and init level.... wich gives a model calibration....


The main purpose of this paper is to propose a new LASSO type penalty
aiming to improve out-of-sample forecasting performance for seasonal time series in
high dimensionality scenarios. We also present results concerning parameter
estimation performance. Our approach leads to what we call SWLadaLASSO
(seasonal weighted lag adaptive LASSO) which assigns larger penalties for higher
lagged covariate coeffcients - based on the idea of WLadaLASSO by Konzen and
Ziegelmann (2016) - except those associated with the seasonal lags of the response
variable. It can be considered a generalization of the WLadaLASSO. In our Monte
Carlo studies, the SWLadaLASSO is superior in terms of forecasting, parameter
estimation and also covariate selection in most of the cases when compared to other
LASSO-type penalty models. An empirical application is conducted to evaluate the
capability of the proposed approach to forecast Brazilian GDP growth. Additionally, a
set of forecast combinations is implemented in search of forecast accuracy
improvement.
